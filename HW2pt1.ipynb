{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79254621-fcbb-42d8-aceb-7a143469054a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure Spark Session for Azure Access and Datetime Compatibility\n",
    "\n",
    "This cell defines two Spark configuration settings required for data access \n",
    "and compatibility across Spark versions.\n",
    "\n",
    "1. Azure Data Lake Access:\n",
    "   - The first configuration registers the access key for the Azure Data Lake \n",
    "     Storage Gen2 account associated with the Goodreads project.\n",
    "   - It enables authenticated read and write operations through the ABFSS protocol.\n",
    "\n",
    "2. Legacy Datetime Parser Policy:\n",
    "   - The second setting restores the legacy time-parsing behavior introduced before \n",
    "     Spark 3.0. This ensures consistent parsing of datetime strings that use \n",
    "     non-standard or locale-specific formats (e.g., those found in the Goodreads data).\n",
    "\n",
    "Note:\n",
    "For production or collaborative use, credentials should be stored securely in \n",
    "Databricks Secrets or Azure Key Vault rather than embedded directly in code.\n",
    "\"\"\"\n",
    "\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.goodreadsreviews60302363.dfs.core.windows.net\",\n",
    "    \"8aeNipwlgfgeg1YnUzDh8PeVxg0I5MmnwgWEORAqG5WIJ4Q/XsFa5m714y55ZfAzUw3nNaEFM/e8+AStXU0APQ==\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdc1ed2-3e9e-44de-8534-f3299fba4cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+--------------------+\n",
      "|           review_id| book_id|               title|author_id|            name|             user_id|rating|         review_text|language|n_votes|          date_added|\n",
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+--------------------+\n",
      "|1b5575e93fafb5a6f...|30739547|Preppy: The Life ...|  5769238|    T.M. Frazier|68f9915717ccc347b...|     5|4 . 5 Bow tie wea...|   en-US| 115329|Sat Jul 30 14:26:...|\n",
      "|c8fb6d2d662d6d498...|13539044|The Silver Lining...|  1251730|   Matthew Quick|b48aade607aa17b3d...|     5|The book was very...|     eng| 201333|Thu Mar 28 07:45:...|\n",
      "|7f7c14c976eb3d645...|29074768|        If I Fix You| 14981314|Abigail  Johnson|d76881f6f75216d6f...|     4|Ah, If I Fix You....|     eng|   1192|Sat Jun 11 20:02:...|\n",
      "|1114d9b34b67d629e...|13790759|Sarah Gives Thank...|  5861865|    Mike Allegra|8fda322a72d04a412...|     5|I LOVED this book...|     eng|    193|Wed Nov 12 20:30:...|\n",
      "|919d2e4c49393efff...|30372977|Full Tilt (Full T...| 10405165|    Emma   Scott|ad1aca4ff11ca84ea...|     5|I received an ARC...|     eng|  20211|Sat Mar 05 19:31:...|\n",
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load Curated Gold Dataset for Verification\n",
    "\n",
    "This cell reads the 'curated_reviews' Delta table from the Gold layer of \n",
    "the Azure Data Lake into a Spark DataFrame named 'silver'.\n",
    "\n",
    "Purpose:\n",
    "    - Validate that the previously written Delta dataset can be accessed successfully.\n",
    "    - Display a sample of records to confirm the integrity and readability of the data.\n",
    "\n",
    "This verification step ensures the curated dataset is correctly stored and ready \n",
    "for subsequent cleaning, transformation, or feature engineering tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Read the Delta table\n",
    "silver = spark.read.format(\"delta\").load(\"abfss://lakehouse@goodreadsreviews60302363.dfs.core.windows.net/gold/curated_reviews/\")\n",
    "\n",
    "# Show few rows\n",
    "silver.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a647fb-2af9-4ff4-b417-f142afe5a091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- n_votes: integer (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+--------------------+\n",
      "|           review_id| book_id|               title|author_id|            name|             user_id|rating|         review_text|language|n_votes|          date_added|\n",
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+--------------------+\n",
      "|1b5575e93fafb5a6f...|30739547|Preppy: The Life ...|  5769238|    T.M. Frazier|68f9915717ccc347b...|     5|4 . 5 Bow tie wea...|   en-US| 115329|Sat Jul 30 14:26:...|\n",
      "|c8fb6d2d662d6d498...|13539044|The Silver Lining...|  1251730|   Matthew Quick|b48aade607aa17b3d...|     5|The book was very...|     eng| 201333|Thu Mar 28 07:45:...|\n",
      "|7f7c14c976eb3d645...|29074768|        If I Fix You| 14981314|Abigail  Johnson|d76881f6f75216d6f...|     4|Ah, If I Fix You....|     eng|   1192|Sat Jun 11 20:02:...|\n",
      "|1114d9b34b67d629e...|13790759|Sarah Gives Thank...|  5861865|    Mike Allegra|8fda322a72d04a412...|     5|I LOVED this book...|     eng|    193|Wed Nov 12 20:30:...|\n",
      "|919d2e4c49393efff...|30372977|Full Tilt (Full T...| 10405165|    Emma   Scott|ad1aca4ff11ca84ea...|     5|I received an ARC...|     eng|  20211|Sat Mar 05 19:31:...|\n",
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Standardize Column Data Types in Curated Dataset\n",
    "\n",
    "This cell enforces consistent data types for all columns in the 'silver' DataFrame\n",
    "to ensure compatibility and accuracy during analytical and transformation operations.\n",
    "\n",
    "Purpose:\n",
    "    - Convert key identifier fields (review_id, book_id, author_id, user_id) to StringType.\n",
    "    - Cast numeric attributes such as 'rating' and 'n_votes' to IntegerType.\n",
    "    - Ensure textual fields (title, name, review_text, language) are explicitly defined as StringType.\n",
    "\n",
    "This step guarantees schema consistency across all data layers and minimizes \n",
    "type-related errors in downstream processing or aggregations.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Adjusting Data Types\n",
    "silver = silver.withColumn(\"review_id\", col(\"review_id\").cast(StringType())) \\\n",
    "       .withColumn(\"book_id\", col(\"book_id\").cast(StringType())) \\\n",
    "       .withColumn(\"author_id\", col(\"author_id\").cast(StringType())) \\\n",
    "       .withColumn(\"user_id\", col(\"user_id\").cast(StringType())) \\\n",
    "       .withColumn(\"rating\", col(\"rating\").cast(IntegerType())) \\\n",
    "       .withColumn(\"n_votes\", col(\"n_votes\").cast(IntegerType())) \\\n",
    "       .withColumn(\"title\", col(\"title\").cast(StringType())) \\\n",
    "       .withColumn(\"name\", col(\"name\").cast(StringType())) \\\n",
    "       .withColumn(\"review_text\", col(\"review_text\").cast(StringType())) \\\n",
    "       .withColumn(\"language\", col(\"language\").cast(StringType())) \\\n",
    "\n",
    "silver.printSchema()\n",
    "silver.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30378fa5-5822-40eb-92b3-a5900a02ba77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+----------+\n",
      "|           review_id| book_id|               title|author_id|            name|             user_id|rating|         review_text|language|n_votes|date_added|\n",
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+----------+\n",
      "|1b5575e93fafb5a6f...|30739547|Preppy: The Life ...|  5769238|    T.M. Frazier|68f9915717ccc347b...|     5|4 . 5 Bow tie wea...|   en-US| 115329|2016-07-30|\n",
      "|c8fb6d2d662d6d498...|13539044|The Silver Lining...|  1251730|   Matthew Quick|b48aade607aa17b3d...|     5|The book was very...|     eng| 201333|2013-03-28|\n",
      "|7f7c14c976eb3d645...|29074768|        If I Fix You| 14981314|Abigail  Johnson|d76881f6f75216d6f...|     4|Ah, If I Fix You....|     eng|   1192|2016-06-12|\n",
      "|1114d9b34b67d629e...|13790759|Sarah Gives Thank...|  5861865|    Mike Allegra|8fda322a72d04a412...|     5|I LOVED this book...|     eng|    193|2014-11-13|\n",
      "|919d2e4c49393efff...|30372977|Full Tilt (Full T...| 10405165|    Emma   Scott|ad1aca4ff11ca84ea...|     5|I received an ARC...|     eng|  20211|2016-03-06|\n",
      "+--------------------+--------+--------------------+---------+----------------+--------------------+------+--------------------+--------+-------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert 'date_added' Column to Standard Date Format\n",
    "\n",
    "This cell standardizes the 'date_added' column by converting its string-based\n",
    "timestamp values into a proper Spark DateType using explicit dateâ€“time parsing.\n",
    "\n",
    "Process:\n",
    "    - The 'to_timestamp()' function interprets the original string pattern \n",
    "      (e.g., \"Tue May 09 14:22:31 +0000 2017\") according to the specified format.\n",
    "    - The 'to_date()' function then extracts and stores only the date component \n",
    "      for simplified temporal analysis.\n",
    "\n",
    "This conversion ensures consistency across all date fields and supports\n",
    "accurate filtering, aggregation, and time-based analysis in later stages.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import to_date, to_timestamp, col\n",
    "\n",
    "silver = silver.withColumn(\n",
    "    \"date_added\",\n",
    "    to_date(to_timestamp(col(\"date_added\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\n",
    ")\n",
    "\n",
    "silver.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871221d8-a256-4f2c-8909-388ec4963503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handle Missing Values in Key Columns\n",
    "\n",
    "This cell performs targeted data cleaning operations to address null or missing \n",
    "values in critical columns of the 'silver' DataFrame.\n",
    "\n",
    "Cleaning Steps:\n",
    "    - Remove records with null ratings, as these cannot contribute to quantitative analysis.\n",
    "    - Replace missing 'n_votes' values with 0 to indicate books or authors with no recorded votes.\n",
    "    - Replace missing 'language' entries with the label \"Unknown\" to preserve data completeness.\n",
    "\n",
    "These transformations improve data integrity, ensuring that the dataset remains\n",
    "consistent and analyzable without introducing bias or invalid records.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Drop rows with no rating\n",
    "silver = silver.filter(col(\"rating\").isNotNull())\n",
    "\n",
    "# Replace missing n_votes with 0\n",
    "silver = silver.withColumn(\"n_votes\", when(col(\"n_votes\").isNull(), lit(0)).otherwise(col(\"n_votes\")))\n",
    "\n",
    "# Replace missing language with \"Unknown\"\n",
    "silver = silver.withColumn(\"language\", when(col(\"language\").isNull(), lit(\"Unknown\")).otherwise(col(\"language\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec57da1e-9483-463d-8c1c-10a3c2a93a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter Invalid Reviews and Compute Review Length\n",
    "\n",
    "This cell enhances data quality by introducing a new derived feature and \n",
    "filtering out unreliable records from the 'silver' DataFrame.\n",
    "\n",
    "Processing Steps:\n",
    "    - Add a new column, 'review_length', which measures the number of characters \n",
    "      in each review's text content.\n",
    "    - Exclude reviews with fewer than 10 characters to remove non-informative entries.\n",
    "    - Remove records with null or invalid 'date_added' values and ensure all dates \n",
    "      fall on or before the current system date.\n",
    "\n",
    "These cleaning steps ensure that only meaningful, time-valid, and analytically useful \n",
    "reviews are retained for downstream analysis.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import length, current_date\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "silver = silver.withColumn(\"review_length\", length(col(\"review_text\")))\n",
    "\n",
    "# Drop reviews with text length < 10\n",
    "silver = silver.filter(col(\"review_length\") >= 10)\n",
    "\n",
    "# Remove invalid or future dates\n",
    "silver = silver.filter((col(\"date_added\").isNotNull()) & (col(\"date_added\") <= current_date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efa1bb4-00cb-40ca-a826-785d1063685f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalize Text Fields for Consistency\n",
    "\n",
    "This cell standardizes text-based columns in the 'silver' DataFrame to ensure\n",
    "uniform formatting and readability across records.\n",
    "\n",
    "Transformations Applied:\n",
    "    - 'trim()' removes leading and trailing whitespace from all text fields.\n",
    "    - 'initcap()' capitalizes the first letter of each word in 'title' and 'name' \n",
    "      to maintain consistent naming conventions.\n",
    "    - 'review_text' is trimmed to eliminate unnecessary spaces without altering case.\n",
    "\n",
    "These formatting adjustments improve data presentation and prevent inconsistencies \n",
    "that may affect matching, grouping, or visualization tasks.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import trim, initcap\n",
    "\n",
    "# Trim and Capitalize Each Word\n",
    "silver = silver.withColumn(\"title\", initcap(trim(col(\"title\")))) \\\n",
    "       .withColumn(\"name\", initcap(trim(col(\"name\")))) \\\n",
    "       .withColumn(\"review_text\", trim(col(\"review_text\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6304bfd1-eff2-4f0d-a787-a5c99f8e985f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save Cleaned and Enriched Dataset to Gold Layer\n",
    "\n",
    "This cell writes the fully cleaned and standardized 'silver' DataFrame to the \n",
    "Gold layer of the Azure Data Lake in Delta format. This step finalizes the data \n",
    "preparation process, making the dataset ready for analysis, reporting, or modeling.\n",
    "\n",
    "Key Parameters:\n",
    "    - format(\"delta\"): Stores the dataset in Delta Lake format for ACID compliance \n",
    "      and efficient querying.\n",
    "    - mode(\"overwrite\"): Replaces any existing version of the dataset in the target directory.\n",
    "    - option(\"overwriteSchema\", \"true\"): Ensures that the stored schema matches \n",
    "      the current DataFrame structure.\n",
    "\n",
    "Destination Path:\n",
    "    abfss://lakehouse@goodreadsreviews60302363.dfs.core.windows.net/gold/features_v1/\n",
    "\n",
    "The resulting Gold-layer dataset represents the final, production-ready version of the \n",
    "Goodreads data pipeline.\n",
    "\"\"\"\n",
    "\n",
    "silver.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"abfss://lakehouse@goodreadsreviews60302363.dfs.core.windows.net/gold/features_v1/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HW2pt1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
